log_level: info
demo_mode: false

incident_store:
  provider: postgresql
  args:
    host: "127.0.0.1"
    database: kai
    user: kai
    password: dog8code

models:
  # **IBM served granite**
  # ```yaml
  #   provider: ChatIBMGenAI
  #   args:
  #     model_id: ibm/granite-13b-chat-v2
  # ```

  # **IBM served mistral**
  # ```yaml
  #   provider: ChatIBMGenAI
  #   args:
  #     model_id: ibm-mistralai/mixtral-8x7b-instruct-v01-q
  # ```

  # **IBM served codellama**
  # ```yaml
  #   provider: ChatIBMGenAI
  #   args:
  #     model_id: meta-llama/llama-2-13b-chat
  # ```

  # **IBM served llama3**
  # ```yaml
  #   # Note:  llama3 complains if we use more than 2048 tokens
  #   # See:  https://github.com/konveyor-ecosystem/kai/issues/172
  #   provider: ChatIBMGenAI
  #   args:
  #     model_id: meta-llama/llama-3-70b-instruct
  #     parameters:
  #       max_new_tokens: 2048
  # ```

  # **Ollama**
  # ```yaml
  #   provider: ChatOllama
  #   args:
  #     model: mistral
  # ```

  # **OpenAI GPT 3.5**
  # ```yaml
  #   provider: ChatOpenAI
  #   args:
  #     model: gpt-4
  # ```

  # **OpenAI GPT 4**
  # ```yaml
  #   provider: ChatOpenAI
  #   args:
  #     model: gpt-3.5-turbo
  # ```
  provider: ChatIBMGenAI
  args:
    model_id: ibm-mistralai/mixtral-8x7b-instruct-v01-q

embeddings:
  todo: true
