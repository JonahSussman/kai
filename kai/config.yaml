log_level: info
demo_mode: false

incident_store:
  provider: postgresql
  args:
    host: "127.0.0.1"
    databse: kai
    user: kai
    password: dog8code

models:
  # How to run with: (look at model_provider.py for more info)
  #   --- IBM served granite ---
  #     provider: IBMGranite
  #     args:
  #       model_id: ibm/granite-13b-chat-v2

  #   --- IBM served mistral ---
  #     provider: IBMOpenSource
  #     args:
  #       model_id: ibm-mistralai/mixtral-8x7b-instruct-v01-q

  #   --- IBM served codellama ---
  #     provider: IBMOpenSource
  #     args:
  #       model_id: meta-llama/llama-2-13b-chat

  #   --- IBM served llama3 ---
  # .   Note:  llama3 complains if we use more than 2048 tokens
  # .   See:  https://github.com/konveyor-ecosystem/kai/issues/172
  #     provider: IBMOpenSource
  #     args:
  #       model_id: meta-llama/llama-3-70b-instruct
  #       parameters:
  #         max_new_tokens: 2048

  #   --- Ollama ---
  #     provider: ChatOllama
  #     args:
  #       model: mistral

  #   --- OpenAI GPT 3.5 ---
  #     provider: ChatOpenAI
  #     args:
  #       model: gpt-4

  #   --- OpenAI GPT 4 ---
  #     provider: ChatOpenAI
  #     args:
  #       model: gpt-3.5-turbo
  provider: ChatIBMGenAI
  args:
    model_id: ibm-mistralai/mixtral-8x7b-instruct-v01-q

embeddings:
  todo: true
